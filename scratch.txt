max outbound / inbound connections or fan-out/fan-in between nodes.
- e.g., 50 node cluster, but node X doesn't talk to all 49 other nodes.
- e.g., 10,000 node cluster, but node X doesn't talk to all 9999 other nodes.

shelf/rack/zone clumping - RI vs R matrices.
z0
 c0.0
  s0.0.0
   a,b,c
  s0.0.1
   d,e
 c0.1
  s0.1.0
   f,g

- first slave on same shelf as master; (0)
    second slave on different shelf, same cabinet; (1)
       third slave on different cabinet. (2)
- first slave on different shelf, same cabinet as master; (1)
    second slave on different cabinet, same zone; (2)
       third slave on different zone. (3)
- first slave on different cabinet, same zone; (2)
    second slave on different shelf, same cabinet as master; (1)
       third slave on different zone. (3)
- same X, but optionally not same X.child as master or slave-0...
       shelf, n/a            1,0
       cabinet, shelf        2,1
       cabinet, n/a          2,0
       zone, cabinet         3,2
       zone, shelf           3,1
       zone, n/a             3,0
       universe, zone        0,3
       universe, cabinent    0,2
       universe, shelf       0,1
       anywhere              0,0
- and, hard versus soft limits

for master-1, w.r.t. master-0, same cabinet, different shelf

for master-1, w.r.t. master-0, same zone, different cabinet

for slave-0, w.r.t. master-0, same cabinet, different shelf
for slave-1, w.r.t. master-0, same zone, different cabinet

for slave-0, w.r.t. master-0, same zone, different cabinet
for slave-1, w.r.t. master-0, same cabinet, different shelf
for slave-2, w.r.t. master-0, same shelf, (different node)

is max fan-out just an edge case of rack/zone tagging?
- same hash clump as master?
- it's kind of like a sub-sorting?

----------------
targetMap: want to reach this state on all nodes & partitions in the cluster.
currentMap: represents the combined view of currentNodeMap's from all nodes.
currentNodeMap: represents the current state of partitions on a single node in the cluster.

actor kinds:
- participant (reads work queue and updates currentState)
- spectator
- conductor

hash-partitioned multiMaster:
"settings": {
  "constraints": {
    "master": 2,
  },
  "numPartitions": 1440,
  "model": "multiMaster",
  "keyFunc": "hash-crc32",
},
"map": {
  "type": "current",
  "numPartitions": 1440,
  "keyFunc": "hash-crc32",
  "partitions": {
    "0": {
      "master": ["nodeA"],
      "slave": ["nodeB"]
    },
    ...
    "1439": {
      "master": ["nodeB"],
      "slave": ["nodeC"]
    }
  }
}

range-partitioned masterSlave:
"settings": {
  "constraints": {
    "master": 2,
  },
  "model": "multiMaster",
  "keyFunc": "range"
},
"map": {
  "type": "target",
  "keyFunc": "range",
  "partitions": {
    "a-c": {
      "source": "a-h",
      "master": ["nodeA", "nodeB"]
    },
    "d-h": {
      "source": "a-h",
      "master": ["nodeA", "nodeB"]
    },
    "i-z": {
      "master": ["nodeA", "nodeB"]
    }
  }
}

how about heterogeneous machines
- some machines are stronger than others and can handle more patitions?
- this is supported by weighting the stateNodeCounts

request to node X: for partition P on node X,
  atomic state change from state S0 to S1.

request between node X and node Y: for partition P, want node X to stream data to node Y,
  with an optional atomic state change ("transition"), where...
    1) node X optionally turns from state S0 into state S1,
    2) node Y optionally turns from state S2 into state S3,
    3) node Y optionally temporarily blocks requests of type R
      until succeess/failure of step 3,
    and state changes use OCC / CAS.

  to handle takeovers,
    when S0 == master, S1 == dead,
         S2 == slave, S3 == master.
  to handle replica,
    when S0 == master, S1 == master,
         S2 == dead, S3 == slave
         or,
         S2 == slave, S3 == slave
    including XDCR and KV-to-BackIndex.

To transition safely from dead to master, need to first become a slave.
To transition safely from master to dead, need to first become a slave.

partition state transitions:
          | n d s m
-------------------
- null    | \ d d d
- dead    | n \ s s
- slave   | d d \ m
- master  | d d d \

pending is a transient / ephemeral state, that helps a takeover of a
master by a slave; on power reboot, state devolves back to slave or
master.

partition running transitions:
          | online | s w r c
----------------------------
- stopped | n      | \ w w w
- warming | y      | c \ r c
- running | y      | c c \ c
- cooling | n      | s s r \

persistence & replication pausing are orthogonal concerns.

healthiness:
- red
- yellow
- green

node:
{
  "volumes": []
}

"config": {
  "maxFullScansPerVolumeType": {
    "ssd": 10,
    "hdd": 1
  }
}

how about proxy/router
- should work
- if have 40,000 nodes, and 40,000 clients,
  then need levels of indirection, or proxy/router.
-- that's just a different map,
   with model of multiMaster, no slaves,
   and R == W == 1 <= N

how about failOvers
- should work
- this is just a slave-to-master state change,
    as long as there are no other masters,
    and

rolling upgrade
- maintenance mode
-- need to hold old maps
- swap rebalance
-- needs to be part of planNewRebalanceMap
- each process should detect when it's fed info that's the wrong
  version window

how about GC'ing dead partitions to null?

how about other replica-like things
- should work
- index
- backIndex
-- need to record the source of each partition
- XDCR
- incremental backup

how about range splitting/repartitioning?
- outside range planner & range executor
- actualizeNewMap needs to know about steps to...

detect when constraints aren't met?
- e.g., number of slaves doesn't meet constraints

